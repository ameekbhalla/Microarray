---
title: "R Notebook"
output: html_notebook
---
# Simulate data
Create a theoretical population
```{r}
set.seed(1)
n.FactA <- 5 #number of levels of A
n.FactB <- 3 #number of levels of B
nsample <- 12 #number of replicates within each level of a factor
n <- n.FactA * nsample #total number of replicates within A

#create factors using gl()
FactA <- gl(n = n.FactA, k = nsample, length = n, lab = paste("a", 1:n.FactA, sep = ""))
FactB <- gl(n = n.FactB, k = nsample/n.FactB, length = n, lab = paste("b", 1:n.FactB, sep = ""))

#Effects
baseline <- 40
FactA.effects <- c(-10, -5, 5, 10)
FactB.effects <- c(5, 10)
interaction.effects <- c(-2, 3, 0, 4, 4, 0, 3, -2)
all.effects <- c(baseline, FactA.effects, FactB.effects, interaction.effects)
X <- as.matrix(model.matrix(~FactA * FactB))

#Theoretical/population means based on the defined effects
RESPONSE1 <- as.numeric(as.matrix(X) %*% as.matrix(all.effects))
theoryMeans <- tapply(RESPONSE1, list(FactA, FactB), mean)
theoryMeans
```

create a sample
```{r}
sigma <- 3
eps <- rnorm(n, 0, sigma)
X <- as.matrix(model.matrix(~FactA * FactB))
RESPONSE <- as.numeric(as.matrix(X) %*% as.matrix(all.effects) + eps)
data <- data.frame(RESPONSE, FactB, FactA)
```

# Calculate sample means
We will now calculate the sample means of each FactA by FactB combination. These are called 'cell means' as they are the means of each cell in a table in which the levels of FactA and FactB are the rows and columns. Thus, there are 15 cell means or unique groups in total [5X3]. If the samples are collected in an unbiased manner, then the cell means should roughly approximate the true means of the underlying populations from which the observations were drawn.

we can estimate the cell means based on a full multiplicative linear model.
The model estimates 15 parameters or coefficients: the intercept (µ; mean value at baseline of each factor), followed by a series of 14 differences.
`model term` `name given in R` `interpretation`
µ 	          (Intercept) 	    (a1b1)
β1 	          FactAa2 	        (a2b1 - a1b1)
β2 	          FactAa3 	        (a3b1 - a1b1)
β3 	          FactAa4 	        (a4b1 - a1b1)
β4 	          FactAa5 	        (a5b1 - a1b1)
β5 	          FactBb2 	        (a1b2 - a1b1)
β6 	          FactBb3 	        (a1b3 - a1b1)
β7 	          FactAa2:FactBb2 	(a2b1+a1b2) - (a1b1+a2b2)
β8 	          FactAa3:FactBb2 	(a3b1+a1b2) - (a1b1+a3b2)
β9 	          FactAa4:FactBb2 	(a4b1+a1b2) - (a1b1+a4b2)
β10 	        FactAa5:FactBb2 	(a5b1+a1b2) - (a1b1+a5b2)
β11 	        FactAa2:FactBb3 	(a2b1+a1b3) - (a1b1+a2b3)
β12 	        FactAa3:FactBb3 	(a3b1+a1b3) - (a1b1+a3b3)
β13 	        FactAa4:FactBb3 	(a4b1+a1b3) - (a1b1+a4b3)
β14 	        FactAa5:FactBb3 	(a5b1+a1b3) - (a1b1+a5b3) 

But this is where the good stuff begins. The linear combination of parameters can now be used to calculate a whole range of related derived estimates (and their standard deviations etc) including:
* population cell means
* marginal means (row and column means)
* effect sizes (differences between marginal means)
* effect sizes (deviations from overal mean)

In order to achieve this wizardry, we must indicate which parameters are relevant for each of the derived estimates required. And this is where **contrast coefficients** come in. The contrast coefficients are a set of numbers (equal in length to the number of model parameters) that determine the combinations and weights of the model parameters that are appropriate for each of the derived estimates. Hence when the set of model parameters are multiplied by a single set of contrast coefficients and then summed up, the result is a new derived estimate. Hence to calculate the cell mean of FactA level a2 and FactB b1 (a2b1), we would specify the first (a1b1 cell mean) and second (difference between a2b1 and a1b1 cell means) contrasts as ones (1) and the others zeros.
We can calculate multiple derived estimates at once, by multiplying the model parameters by multiple sets of contrast coefficients (represented as a matrix). That is, matrix multiplication. 

At this point we could go on and attempt to work out which combinations of model parameters would be required to serve as the set of contrast coefficients for each of the cell means (the cell here means a group derived by a unique combination of the factors). Whilst it is reasonably straight forward to do this for cells a1b1, a2b1, a3b1, a4b1, a5b1, a1b2, a1b3 (those along the top and down the left column), it requires more mental gymnastics to arrive at the entire contrast matrix [each column of the matrix is a vector of contrast coefficents corresponding to one group or cell]. However, there is a function in R called model.matrix() that helps us achieve this in a relatively painfree manner. The model.matrix() function gives the design matrix that is used in linear modelling to indicate group membership of an observation into one of the categorical variables or their combination. If we ignore all the replicate observations present in each group, then we could reduce this matrix down to the level of the groups, thereby generating the contrast matrix. The model.matrix can be reduced to its unique groups in two ways. As the model contains only categorical predictors, we can just return a unique version (one without duplicates), using the function `unique()`. A more universal technique is to aggregate (summarize) the design matrix according to the factor levels. This technique handles models that also include continuous covariates.

The estimated sample cell means are thence derived by multiplying the model parameters by this transposed contrast matrix.

```{r}

data.lm <- lm(RESPONSE ~ FactA * FactB, data)

mat <- unique(model.matrix(data.lm))

# matrix multiplication of model parameters with the transposed contrast matrix (such that contrast coefficeints for each derived estimate are in a column)
modelMeans <- matrix(data.lm$coef %*% t(mat), ncol = length(levels(data$FactB)), byrow = TRUE)

#We can make the presentation of these numbers a little more palatable by arranging them in a table and labeling the rows and columns.
dimnames(modelMeans) <- list(levels(data$FactA), levels(data$FactB))
modelMeans

#compare the above method to computing the means manually by group
groupMeans <- with(data, tapply(RESPONSE, list(FactA, FactB), mean))
modelMeans
```

The same framework can be applied to more complex designs (such as nested, blocking, repeated measures - mixed effects models) for which simple sample means [obtained by using the tapplyt function `with(data, tapply(RESPONSE, list(FactA, FactB), mean))`] are unlikely to be as good at estimating true population means as estimates that are informed by all the FactA by Fact B trends across nesting levels. Furthermore, the model (and contrast sets) allows us to get estimates of standard deviations, standard errors and confidence intervals around the derived estimates by using matrix algebra. 
In order to make use of matrix algebra, the following formatting conditions must be adhered to:
* the contrast matrix MUST be a matrix not a data frame
* each column of the contrast matrix represents a single separate derived parameter 
* there must be exactly the same number of rows as there are parameters in the model

# Calculate standard deviations, standard errors and confidence intervals
Multiplying the contrast matrix by the model parameters derives new parameters (as we have just seen). Multiplying the same contrast matrix by the model variance-covariance matrix derives the associated standard errors (and thus confidence intervals). The variance-covariance matrix contains the variances of the estimated model parameters along the diagonals and the covariances between each parameter pair in the off-diagonals.

Rather than produce the standard errors and confidence intervals in isolation, it is more useful to combine them into a frame along with the derived parameters aka estimates (cell means in this case).
If we add columns to this frame that indicate the levels of each of the factors, we can hen create an interaction plot that incorporates confidence intervals in addition to the cell mean estimates.
```{r}
mat <- unique(model.matrix(data.lm))
modelMeans <- data.lm$coef %*% t(mat)
se <- sqrt(diag(mat %*% vcov(data.lm) %*% t(mat)))

# the function outer() multiplies each of the elements of one array with each of the elements of another array. This is called the outer product of the two arrays. In the line below, outer() is used to multiply each of the standard error estimates by the upper and lower critical values for a 95% confidence interval from a normal distribution 'qnorm()' or a t-distribtuion 'qt()' 

# confidence intervals based on the t-distribution can be calculated from standard errors according to the following equation: 95% CI = μ ± t0.05,df × SE where df is the t-distribution's degrees of freedom.For more complex designs (particularly those that are hierarchical), identifying or even approximating sensible estimates of degrees of freedom (and thus the t-distributions from which to estimate confidence intervals) becomes increasingly more problematic (and some argue, borders on guess work). As a result, more complex models do not provide any estimates of residual degrees of freedom and thus, it is necessary to either base confidence intervals on the normal distribution

# ci <- as.numeric(modelMeans) + outer(se, qnorm(c(0.025, 0.975)))
ci <- as.numeric(modelMeans) + outer(se, qt(df = data.lm$df, c(0.025, 0.975)))
colnames(ci) <- c("lwr", "upr")
mat2 <- cbind(mu = as.numeric(modelMeans), se = se, ci)
rownames(mat2) <- nms
mat2

mat2 <- data.frame(with(data, expand.grid(FactB = levels(FactB), FactA = levels(FactA))), mat2)

# Interaction plot: If we add columns to this frame that indicate the levels of each of the factors, we can then create an interaction plot that incorporates confidence intervals in addition to the cell mean estimates.

mat2 <- data.frame(with(data, expand.grid(FactB = levels(FactB), FactA = levels(FactA))), mat2)
```


# Marginal means
The true flexibility of the contrasts is the ability to derive other estimates, such as the marginal means for each level of a factor. The marginal mean for a level is obtained by averaging over all the levels of the other factor (and also adding in the average of the relevant interactions).The marginal means are the means of rows and columns in a table in which the levels of FactA and FactB are the rows and columns 
```{r}
# library(plyr)
mat <- model.matrix(~FactA * FactB, data)
dat <- cbind(data, mat)

# FactA marginal means
# cmat <- ddply(dat, ~FactA, function(df) mean(df[, -1:-3])) #remove the first three
cms <- loadWorkbook("cm.xlsx")
cmat <- readWorksheet(cms, 1)
# We need to convert this into a matrix & remove the 1st column (which was only used to help aggregate)
cmat <- as.matrix(cmat[, -1])
# Use this contrast matrix to derive new parameters and their standard rrors
FactAs <- data.lm$coef %*% t(cmat)
se <- sqrt(diag(cmat %*% vcov(data.lm) %*% t(cmat)))
ci <- as.numeric(FactAs) + outer(se, qt(df = data.lm$df, c(0.025, 0.975)))
colnames(ci) <- c("lwr", "upr")
mat2 <- cbind(mu = as.numeric(FactAs), se, ci)
rownames(mat2) <- levels(data$FactA)

# FactB marginal means
# cmat <- ddply(dat, ~FactB, function(df) mean(df[, -1:-3]))
cms <- loadWorkbook("cm.xlsx")
cmat <- readWorksheet(cms, 2)
cmat <- as.matrix(cmat[, -1])
FactBs <- data.lm$coef %*% t(cmat)
se <- sqrt(diag(cmat %*% vcov(data.lm) %*% t(cmat)))
ci <- as.numeric(FactBs) + outer(se, qt(df = data.lm$df, c(0.025, 0.975)))
colnames(ci) <- c("lwr", "upr")
mat2 <- cbind(mu = as.numeric(FactBs), se, ci)
rownames(mat2) <- levels(data$FactB)

#FactA by FactB interaction marginal means 
#NOTE, as there are only two factors, this gives the cell means
#[it only makes sense to calculate this if there are at least three factors?]

#cmat <- ddply(dat, ~FactA * FactB, function(df) mean(df[, -1:-3]))
cmat <- readWorksheet(cms, 3)
cmat <- as.matrix(cmat[, -1])
FactAFactBs <- data.lm$coef %*% t(cmat)
se <- sqrt(diag(cmat %*% vcov(data.lm) %*% t(cmat)))
ci <- as.numeric(FactAFactBs) + outer(se, qt(df = data.lm$df, c(0.025, 0.975)))
colnames(ci) <- c("lwr", "upr")
mat2 <- cbind(mu = as.numeric(FactAFactBs), se, ci)
rownames(mat2) <- interaction(with(data, expand.grid(levels(FactB), levels(FactA))[, c(2, 1)]), sep = "")
```

# Effect sizes - differences between (marginal) means
An effect size is a difference between two parameters. To derive the contrast set for an effect size, you simply subtract one contrast set from another and use the resulting as the contrast set. For example, the simple effect of a1 vs a2 at b1 would have its contrast set generated when the contrast set of a1b1 is subtract the contrast set of a2b1. Alternatively, we can even calculate the marginal effect sizes instaed of simple effect sizes (difference between a1 averaged across b and a2 averaged across b). Whilst it is possible to go through and calculate all the marginal effects individually, we can loop through and create a matrix that contains all the appropriate contrasts.

```{r}
# #Effect size for simple effect of a1b1-a2b1
# mat <- unique(model.matrix(data.lm))
# mat1 <- mat[1, ] - mat[2, ]
# effectSizes <- data.lm$coef %*% mat1
```

```{r}
#Effect sizes for marginal effects of all FactA levels:
# cmat <- ddply(dat, ~FactA, function(df) mean(df[, -1:-3]))
cms <- loadWorkbook("cm.xlsx")
cmat <- readWorksheet(cms, 1)
cmat <- as.matrix(cmat[, -1])
cm <- NULL
for (i in 1:(nrow(cmat) - 1)) {
  for (j in (i + 1):nrow(cmat)) {
nm <- paste(levels(data$FactA)[j],"−",levels(data$FactA)[i])
eval(parse(text = paste("cm <- cbind(cm,'", nm, "'=c(cmat[j,]-cmat[i,]))")))
  }
  }
FactAEffects <- data.lm$coef %*% cm
se <- sqrt(diag(t(cm) %*% vcov(data.lm) %*% cm))
ci <- as.numeric(FactAEffects) + outer(se, qnorm(c(0.025, 0.975)))
colnames(ci) <- c("lwr", "upr")
mat2 <- cbind(mu = as.numeric(FactAEffects), se, ci)

# Effect sizes plot
plot(seq(-22, 22, l = nrow(mat2)), 1:nrow(mat2), type = "n", ann = F, axes = F)
abline(v = 0, lty = 2)
segments(mat2[, "lwr"], 1:nrow(mat2), mat2[, "upr"], 1:nrow(mat2))
points(mat2[, "mu"], 1:nrow(mat2), pch = 16)
text(mat2[, "lwr"], 1:nrow(mat2), lab = rownames(mat2), pos = 2)
axis(1)

#Effect sizes for marginal effects of all FactB levels:
# cmat <- ddply(dat, ~FactB, function(df) mean(df[, -1:-3]))
cms <- loadWorkbook("cm.xlsx")
cmat <- readWorksheet(cms, 2)
cmat <- as.matrix(cmat[, -1])
cm <- NULL
for (i in 1:(nrow(cmat) - 1)) {
  for (j in (i + 1):nrow(cmat)) {
    nm <- paste(levels(data$FactB)[j],"−",levels(data$FactB)[i])
    eval(parse(text = paste("cm <- cbind(cm,'", nm, "'=c(cmat[j,]-cmat[i,]))")))
  }
  }
FactBEffects <- data.lm$coef %*% cm
se <- sqrt(diag(t(cm) %*% vcov(data.lm) %*% cm))
ci <- as.numeric(FactBEffects) + outer(se, qnorm(c(0.025, 0.975)))
colnames(ci) <- c("lwr", "upr")
mat2 <- cbind(mu = as.numeric(FactBEffects), se, ci)
```

# Effect sizes - simple effects - effects of one factor at specific levels of another factor
But what do we do in the presence of an interaction then - when effect sizes of marginal means [eg B3 average over all levels of A minus B1 averaged over all levels of A] will clearly be unrepresentative of effect sizes at different levels [eg B3 at A1 minus B1 at A1] . The presence of an interaction implies that the effect sizes differ according to the combinations of the factor levels.
```{r}
# simple effects of b3 vs b1 at each level of Factor A [output: 1X5]
fits <- NULL
for (fA in levels(data$FactA)) {
     mat <- model.matrix(~FactA * FactB, data[data$FactA == fA & data$FactB %in% c("b1", "b3"), ])
     mat <- unique(mat)
     dat <- data[data$FactA == fA & data$FactB %in% c("b1", "b3"), ]
     dat$FactB<−factor(dat$FactB)
     cm <- NULL
     for (i in 1:(nrow(mat) - 1)) {
         for (j in (i + 1):nrow(mat)) {
             nm <- with(dat, paste(levels(FactB)[j], "-", levels(FactB)[i]))
             eval(parse(text = paste("cm <- cbind(cm,'", nm, "'=c(mat[j,]-mat[i,]))")))
         }
     }
     es <- data.lm$coef %*% cm
     se <- sqrt(diag(t(cm) %*% vcov(data.lm) %*% cm))
     ci <- as.numeric(es) + outer(se, qnorm(c(0.025, 0.975)))
     mat2 <- cbind(mu = as.numeric(es), se, ci)
     fits <- rbind(fits, mat2)
 }
 colnames(fits) <- c("ES", "se", "lwr", "upr")
 rname <- rownames(fits)
 fits <- data.frame(fits)
 fitsFactA<−levels(data$FactA)
 fits$Comp <- factor(rname)
 fits

# simple effects of b2 vs b1 at each level of Factor A
# simple effects of b3 vs b1 at each level of Factor A [as above]
# simple effects of b3 vs b2 at each level of Factor A
# [output: 3X5]
 
fits <- NULL
for (fA in levels(data$FactA)) {
     cc <- combn(levels(data$FactB), 2)
     for (nfb in 1:ncol(cc)) {
         dat <- data[data$FactA == fA & data$FactB %in% cc[, nfb], ]
         mat <- model.matrix(~FactA * FactB, data = dat)
         mat <- unique(mat)
         dat$FactB<−factor(dat$FactB)
         cm <- NULL
         for (i in 1:(nrow(mat) - 1)) {
             for (j in (i + 1):nrow(mat)) {
                 nm <- with(dat, paste(levels(FactB)[j], "-", levels(FactB)[i]))
                 eval(parse(text = paste("cm <- cbind(cm,'", nm, "'=c(mat[j,]-mat[i,]))")))
             }
         }
         es <- data.lm$coef %*% cm
         se <- sqrt(diag(t(cm) %*% vcov(data.lm) %*% cm))
         ci <- as.numeric(es) + outer(se, qnorm(c(0.025, 0.975)))
         mat2 <- cbind(mu = as.numeric(es), se, ci, fA)
         mat2 <- data.frame(mu = as.numeric(es), se, ci, fA, Comp = rownames(mat2))
         fits <- rbind(fits, mat2)
     }
 }
 colnames(fits) <- c("ES", "se", "lwr", "upr", "FactA", "Comp")
 fits
```

References:
1. http://www.flutterbys.com.au/stats/tut/tut7.7.html#h4_5

2. http://www.flutterbys.com.au/stats/ws/ws11.html#Q5